% -----------------------------------------------
% Template for ISMIR 2010
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2010,amsmath,cite}
\usepackage{graphicx}
\usepackage{url}
\usepackage{algorithm,algorithmic}


% Title.
% ------
\title{Large-scale harmonic patterns clustering}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
% what order do we use? Thierry, Ron, Dan? is Juan on?


\begin{document}
%
\maketitle
%
\begin{abstract}
We present a method for discovering typical patterns in music.
Similar to the \textit{shingle} idea, but done on a larger scale through
Echo Nest API, we cluster harmonic patterns from 43K songs. Patterns are
made of ordered chroma features, and not of summarization like gaussian
mixtures coefficients or NMF weights. We explain
how to operate on such large datasets, we analyze the obained clusters,
and we discuss there use for retrieval and encoding. All code is made
available.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

The original goal of the \textit{Shingles} was retrieval, i.e. find very
similar song segments based on their harmonic pattern. Part of the
challenge was to handle tons of overlapping song segments represented
by a rather long feature vector (a serie of chroma features).
The LSH algorithm proved a great solution for performing a $k$-nearest
neighbors ($k$nn) search in such a space. However, the paradigm of $k$nn has
an intrinsec scaling limit.

Finding a closest match is a tangible goal within a personal collection
of a few thousands songs. It can be used to discover cover songs or
find the seamless transitions in a playlist. However, the closest match
in millions and millions of music excerpts do not help us organize such
an amount of media. Some tasks are still important, for instance cover
recognition, and improvements in $k$nn methods are always welcome.
But it does not the describe the music space, nor does it necessarily
make good recommendations. A similar argument in the recommendation
domain is that precision becomes more important than recall passed a certain
scale of data for many practical uses.

We propose to move the \textit{Shingle} idea into the larger clustering paradigm.
The goal is to find typical patterns that can be seen as cluster centroids.
Those typical patterns help us understand a music collection from an
harmonic point of view. Segments of songs can be described in regard to their
proximity to the different centroids, thus creating some notion of distance
among the song segments. These cluster of similar patterns can have many usages:
\begin{enumerate}
\item describing the music space
\item finding potential similar songs and/or covers
\item eliminating quickly very wrong matches in a query system
\end{enumerate}

We present a practical way to do such large-scale clustering using vector
quantization. We present clustering results of an online algorithm on a data 
set of $43$K songs, but the method applies to any data size. We show how to
select different parameters such as the size of the \textit{Shingles} 
using an encoding scheme on a test set of $4$K songs. Finally, we illustrate
the method with some specific examples.


\section{Previous Work}\label{sec:prevwork}
The idea is closely related to the \textit{Shingles} described by
Slaney and Casey \cite{Casey2006,Casey2007,Casey2008}. 
LSH: \cite{Datar2004}, LSH implementation: \cite{E2LSH}.
Barrington et al. recently studied music texture using a set of ordered
MFCC vectors \cite{Barrington2009a}. 
Nearest neighbor for music also includes \cite{Cano2004} and ...

Other large-scale work use summarization of the features. We mention 
gaussian mixtures \cite{Mandel2005}, aggregate features \cite{Bergstra2006a}
and linear dynamical systems \cite{Barrington2009a}. The chroma representation
is close from an harmonic point of view to the actual music, in particular
we can ``listen'' to the patches (see Section \ref{sec:experiments}).
This is an advantage for analyzing a large data set, and in accordance with our goal
we restrain ourselves from using generative models to compress our features.


\section{Data}\label{sec:data}

\subsection{Echo Nest features}
We use the Echo Nest analyze API \cite{EchoNest} throughout this work.
The API gives us a chroma vector (length $12$) for every music event, 
or ``segment'', for any song we upload to their platform. 
We also get an estimate of where the beats and the bars are. 
We transform the chroma vectors per segment into per beat vector using a 
simple average. We can then stack beat vectors for a number of bars (usually 1 or 2). 
We get a fixed-size patch by resampling the number of chroma vectors. Typically,
there are $4$ beats per bar. Thus an appropriate patch size for one bar
is $12$x$4$ or $12$x$8$.

Note that we do not claim that any of these informations (segments, beats, bars)
are perfectly accurate. Practise showed us that they are reasonable, and the
size of the data set should make up for the imperfections or noise.
We also believe that patches sizes based on a number of beats or bars are more
meaningful than an arbitrary time length. More on this in the experiments
(Section \ref{sec:experiments}).


\begin{figure}[htb]
\begin{center}
\includegraphics[width=.8\columnwidth]{code}
\end{center}
\caption{{A typical code from a code book of size $200$. Patterns represented
$2$ bars and the pattern length was set to $16$.}}
\label{fig:code}
\end{figure}

\subsection{Cowbell Dataset}
We have $3,720,091$ non zero bars, a bar usually contains
$4$ beats.


\subsection{uspop2002}
We had acces to a low quality (32kbps) version of the songs in the uspop 2002 data
set \cite{uspop2002}.
This well-known data show a great diversity of pop songs, but is not particular
otherwise. We also tried to use Tzanetakis dataset \cite{Tzanetakis2002a}, but 
the $30$ seconds segments seemed hard to analyze by the Echo Nest API, most did
not contain any bar.

\section{Algorithm}\label{sec:algo}

Vector quantization \cite{Gersho1991}. Online learning.

\subsection{Codebook Learning}
We initialize the codebook by choosing $K$ random points from our dataset.


\begin{algorithm}
\caption{Pseudocode of Vector Quantization}
\label{algo:vq}
\begin{algorithmic}
\STATE$l$ learning rate
\STATE$\{P_n\}$ set of patches
\STATE$\{C_k\}$ codebook of $K$ codes
%\STATE $m \leftarrow b$
\REQUIRE $0 < l \leq 1$
\FOR{$nIters$}
\FOR{$p \in \{P_n\}$}
\STATE$c \leftarrow min_{c \in C_k} dist(p,c)$
\STATE$c \leftarrow c + (p - c) * l$
\ENDFOR
\ENDFOR
\RETURN $\{C_k\}$
\end{algorithmic}
\end{algorithm}




\section{Experiments}\label{sec:experiments}

\subsection{Setting}
We take one or two bars, normalize the patches to size 4, 8, or 16.
We roll the patch to be invariant to the key (on the patch level, not on
the song level). We learn a codebook of size $K$ over the cowbell dataset 
using the VQ algorithm (Algorithm \ref{algo:vq}). A typical learning rate 
is $1e-2$ for $20$ iterations over the whole dataset.

We use the codebook to encode our testing set (see Section \ref{sec:data}).
Each pattern is encoded with only one code. We can measure the average
distance between a pattern and its encoding. We can also measure the use
of the different codes.

\subsection{Pattern length}

\subsection{Visualization}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=.9\columnwidth]{codes_lle}
\end{center}
\caption{{LLE visualization of the code book.}}
\label{fig:lle}
\end{figure}

\subsection{Specific Examples}


\section{Conclusion}
We are awesome. Here's how:
\begin{itemize}
\item large-scale
\item smarter shingles, based on beats and bars
\item clusters good for encoding
\item clusters good for something else?
\item free online API, code available\footnote{code not released yet to preserve 
submission's anonymity}
\item we have lab t-shirts
\end{itemize}

As for enhancements, we specifically did not compress our features using
gaussian mixtures or other generative model. That being said, these methods
could be use to develop better distance measures between \textit{Shingles}.
The use of the euclidean distance is arbitrary. Summarizing patches
with gaussians, and then comparing the distance between those gaussians,
could reduce the influence of the noise in the distance measure.



\small
\section{Acknowledgements}
Thierry is NSERC graduate fellow, or some title like that.


%\begin{thebibliography}{citations}
%\bibitem{Someone:04} 
%X. Someone and Y. Someone:
%{\it Title of the Book},
%Editorial Acme, Utrecht, 2004.
%\end{thebibliography}

\bibliography{tbm_bib}





\end{document}
