% -----------------------------------------------
% Template for ISMIR 2010
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2010,amsmath,cite}
\usepackage{graphicx}
\usepackage{url}
\usepackage{algorithm,algorithmic}


% Title.
% ------
\title{Large-scale harmonic patterns clustering}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
% what order do we use? Thierry, Ron, Dan? is Juan on?


\begin{document}
%
\maketitle
%
\begin{abstract}
We present a method for discovering typical patterns in music.
Similar to the \textit{shingle} idea, but done on a larger scale through
Echo Nest API, we cluster harmonic patterns from 43K songs. Patterns are
made of ordered chroma features, and not of summarization like gaussian
mixtures coefficients or NMF weights. We explain
how to operate on such large datasets, we analyze the obained clusters,
and we discuss there use for retrieval and encoding. All code is made
available.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

The original goal of the \textit{Shingles} was retrieval, i.e. find very
similar song segments based on their harmonic pattern. Part of the
challenge was to handle tons of overlapping song segments represented
by a rather long feature vector (a serie of chroma features).
The LSH algorithm proved a great solution for performing a $k$-nearest
neighbors ($k$nn) search in such a space. However, the paradigm of $k$nn has
an intrinsec scaling limit.

Finding a closest match is a tangible goal within a personal collection
of a few thousands songs. It can be used to discover cover songs or
find the seamless transitions in a playlist. However, the closest match
in millions and millions of music excerpts do not help us organize such
an amount of media. Some tasks are still important, for instance cover
recognition, and improvements in $k$nn methods are always welcome.
But it does not the describe the music space, nor does it necessarily
make good recommendations. A similar argument in the recommendation
domain is that precision becomes more important than recall passed a certain
scale of data for many practical uses.

We propose to move the \textit{Shingle} idea into the larger clustering paradigm.
The goal is to find typical patterns that can be seen as cluster centroids.
Those typical patterns help us understand a music collection from an
harmonic point of view. Segments of songs can be described in regard to their
proximity to the different centroids, thus creating some notion of distance
among the song segments. These cluster of similar patterns can have many usages:
\begin{enumerate}
\item describing the music space
\item finding potential similar songs and/or covers
\item eliminating quickly very wrong matches in a query system
\end{enumerate}

We present a practical way to do such large-scale clustering using vector
quantization. We present clustering results of an online algorithm on a data 
set of $43$K songs, but the method applies to any data size. We show how to
select different parameters such as the size of the \textit{Shingles} 
using an encoding scheme on a test set of $4$K songs. Finally, we illustrate
the method with some specific examples.


\section{Previous Work}\label{sec:prevwork}
The idea is closely related to the \textit{Shingles} described by
Slaney and Casey \cite{Casey2006,Casey2007,Casey2008}. 
LSH implementation: \cite{E2LSH}.
Barrington et al. recently studied music texture using a set of ordered
MFCC vectors \cite{Barrington2009a}. 
Nearest neighbor for music also includes \cite{Cano2004} and ...

Other large-scale work use summarization of the features. We mention 
gaussian mixtures \cite{Mandel2005}, aggregate features \cite{Bergstra2006a}
and linear dynamical systems \cite{Barrington2009a}. The chroma representation
is close from an harmonic point of view to the actual music, in particular
we can ``listen'' to the patches (see Section \ref{sec:experiments}).
This is an advantage for analyzing a large data set, and in accordance with our goal
we restrain ourselves from using generative models to compress our features.


\section{Data}\label{sec:data}

\subsection{Echo Nest features}
Echo Nest analyze API: \cite{EchoNest}.

\subsection{Cowbell Dataset}
We have $3,720,091$ non zero bars, a bar usually contains
$4$ beats.


\subsection{uspop2002}
uspop: \cite{uspop2002}.


\section{Algorithm}\label{sec:algo}

Vector quantization. Online learning.

\subsection{Codebook Learning}
We initialize the codebook by choosing $K$ random points from our dataset.


\begin{algorithm}
\caption{Pseudocode of Vector Quantization}
\label{alg:vq}
\begin{algorithmic}
\STATE$l$ learning rate
\STATE$\{P_n\}$ set of patches
\STATE$\{C_k\}$ codebook of $K$ codes
%\STATE $m \leftarrow b$
\REQUIRE $0 < l \leq 1$
\FOR{$nIters$}
\FOR{$p \in \{P_n\}$}
\STATE$c \leftarrow min_{c \in C_k} dist(p,c)$
\STATE$c \leftarrow c + (p - c) * l$
\ENDFOR
\ENDFOR
\RETURN $u$
\end{algorithmic}
\end{algorithm}




\section{Experiments}\label{sec:experiments}





\section{Conclusion}
We are awesome. Here's how:
\begin{itemize}
\item large-scale
\item smarter shingles, based on beats and bars
\item clusters good for encoding
\item clusters good for something else?
\item free online API, code available\footnote{code not released yet to preserve 
submission's anonymity}
\item we have lab t-shirts
\end{itemize}

As for enhancements, we specifically did not compress our features using
gaussian mixtures or other generative model. That being said, these methods
could be use to develop better distance measures between \textit{Shingles}.
The use of the euclidean distance is arbitrary. Summarizing patches
with gaussians, and then comparing the distance between those gaussians,
could reduce the influence of the noise in the distance measure.



\small
\section{Acknowledgements}
Thierry is NSERC graduate fellow, or some title like that.


%\begin{thebibliography}{citations}
%\bibitem{Someone:04} 
%X. Someone and Y. Someone:
%{\it Title of the Book},
%Editorial Acme, Utrecht, 2004.
%\end{thebibliography}

\bibliography{tbm_bib}





\end{document}
